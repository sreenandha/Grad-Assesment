{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fL9MXfHTDEAV",
        "outputId": "b270838c-2f84-46b4-db63-43ee24ccc0a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: conllu in /usr/local/lib/python3.10/dist-packages (6.0.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U conllu datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "Nqf2SzOADQfB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_data = load_dataset(\"universal_dependencies\", \"en_ewt\")\n",
        "german_data = load_dataset(\"universal_dependencies\", \"de_gsd\")\n",
        "\n",
        "print(english_data)\n",
        "print(german_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_phwNnRDSBq",
        "outputId": "574e576e-8613-4335-bf73-99e5d46b9f86"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],\n",
            "        num_rows: 12543\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],\n",
            "        num_rows: 2002\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],\n",
            "        num_rows: 2077\n",
            "    })\n",
            "})\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],\n",
            "        num_rows: 13814\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],\n",
            "        num_rows: 799\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['idx', 'text', 'tokens', 'lemmas', 'upos', 'xpos', 'feats', 'head', 'deprel', 'deps', 'misc'],\n",
            "        num_rows: 977\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ArcStandardParser:\n",
        "    def __init__(self):\n",
        "        self.stack = []\n",
        "        self.buffer = []\n",
        "        self.dependencies = []\n",
        "\n",
        "    def initialize(self, sentence):\n",
        "        self.stack = [(0, \"ROOT\")]  # Root represented as index 0\n",
        "        self.buffer = [(i + 1, token) for i, token in enumerate(sentence)]\n",
        "        self.dependencies = []\n",
        "\n",
        "    def shift(self):\n",
        "        if self.buffer:\n",
        "            self.stack.append(self.buffer.pop(0))\n",
        "\n",
        "    def left_arc(self):\n",
        "        if len(self.stack) >= 2:\n",
        "            dependent = self.stack.pop(-2)\n",
        "            head = self.stack[-1]\n",
        "            self.dependencies.append((head[0], dependent[0]))\n",
        "\n",
        "    def right_arc(self):\n",
        "        if len(self.stack) >= 2:\n",
        "            dependent = self.stack.pop(-1)\n",
        "            head = self.stack[-1]\n",
        "            self.dependencies.append((head[0], dependent[0]))\n",
        "\n",
        "    def parse_step(self, action):\n",
        "        if action == \"SHIFT\":\n",
        "            self.shift()\n",
        "        elif action == \"LEFT-ARC\":\n",
        "            self.left_arc()\n",
        "        elif action == \"RIGHT-ARC\":\n",
        "            self.right_arc()\n",
        "\n",
        "    def get_state(self):\n",
        "        return {\n",
        "            \"stack\": self.stack,\n",
        "            \"buffer\": self.buffer,\n",
        "            \"dependencies\": self.dependencies,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "dpsZ6bRGDV0K"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dependency_to_actions(sentence, heads):\n",
        "    stack = [(0, \"ROOT\")]\n",
        "    buffer = []\n",
        "    for i, token in enumerate(sentence):\n",
        "        buffer.append((i + 1, token))\n",
        "    actions = []\n",
        "\n",
        "    dependents = {}\n",
        "    for i in range(len(sentence) + 1):  # Include ROOT\n",
        "        dependents[i] = []\n",
        "\n",
        "    for i, head in enumerate(heads):\n",
        "        dependent_token_index = i + 1  # 1-based index for tokens\n",
        "        dependents[head].append(dependent_token_index)\n",
        "\n",
        "    created_arcs = set()\n",
        "\n",
        "    while buffer or len(stack) > 1:\n",
        "        if len(stack) >= 2:\n",
        "            s1 = stack[-1][0]\n",
        "            s2 = stack[-2][0]\n",
        "\n",
        "            if s1 in dependents[s2]:\n",
        "                all_dependents_processed = True\n",
        "                for d in dependents[s1]:\n",
        "                    if d not in created_arcs:\n",
        "                        all_dependents_processed = False\n",
        "                        break\n",
        "                if all_dependents_processed:\n",
        "                    actions.append(\"RIGHT-ARC\")\n",
        "                    created_arcs.add(s1)\n",
        "                    stack.pop()\n",
        "                    continue\n",
        "\n",
        "            if s2 in dependents[s1]:\n",
        "                all_dependents_processed = True\n",
        "                for d in dependents[s2]:\n",
        "                    if d not in created_arcs:\n",
        "                        all_dependents_processed = False\n",
        "                        break\n",
        "                if all_dependents_processed:\n",
        "                    actions.append(\"LEFT-ARC\")\n",
        "                    created_arcs.add(s2)\n",
        "                    stack.pop(-2)\n",
        "                    continue\n",
        "\n",
        "        if buffer:\n",
        "            actions.append(\"SHIFT\")\n",
        "            next_token = buffer.pop(0)\n",
        "            stack.append(next_token)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return actions"
      ],
      "metadata": {
        "id": "ezsjiNoWDY3q"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_parser():\n",
        "    sentence = [\"I\", \"like\", \"cats\"]\n",
        "    heads = [2, 0, 2]  # 1-based indices: I->like, like->ROOT, cats->like\n",
        "\n",
        "    print(\"Testing dependency_to_actions:\")\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Heads: {heads}\")\n",
        "\n",
        "    actions = dependency_to_actions(sentence, heads)\n",
        "    print(f\"Generated actions: {actions}\")\n",
        "\n",
        "    parser = ArcStandardParser()\n",
        "    parser.initialize(sentence)\n",
        "    print(\"\\nRunning actions through parser:\")\n",
        "    print(f\"Initial state: {parser.get_state()}\")\n",
        "\n",
        "    for action in actions:\n",
        "        parser.parse_step(action)\n",
        "        print(f\"After {action}: {parser.get_state()}\")\n",
        "\n",
        "# Run test\n",
        "test_parser()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9UdVYApDyBS",
        "outputId": "fa2ac68b-87f5-45a6-c096-3aad8e027dc0"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing dependency_to_actions:\n",
            "Sentence: ['I', 'like', 'cats']\n",
            "Heads: [2, 0, 2]\n",
            "Generated actions: ['SHIFT', 'SHIFT', 'LEFT-ARC', 'SHIFT', 'RIGHT-ARC', 'RIGHT-ARC']\n",
            "\n",
            "Running actions through parser:\n",
            "Initial state: {'stack': [(0, 'ROOT')], 'buffer': [(1, 'I'), (2, 'like'), (3, 'cats')], 'dependencies': []}\n",
            "After SHIFT: {'stack': [(0, 'ROOT'), (1, 'I')], 'buffer': [(2, 'like'), (3, 'cats')], 'dependencies': []}\n",
            "After SHIFT: {'stack': [(0, 'ROOT'), (1, 'I'), (2, 'like')], 'buffer': [(3, 'cats')], 'dependencies': []}\n",
            "After LEFT-ARC: {'stack': [(0, 'ROOT'), (2, 'like')], 'buffer': [(3, 'cats')], 'dependencies': [(2, 1)]}\n",
            "After SHIFT: {'stack': [(0, 'ROOT'), (2, 'like'), (3, 'cats')], 'buffer': [], 'dependencies': [(2, 1)]}\n",
            "After RIGHT-ARC: {'stack': [(0, 'ROOT'), (2, 'like')], 'buffer': [], 'dependencies': [(2, 1), (2, 3)]}\n",
            "After RIGHT-ARC: {'stack': [(0, 'ROOT')], 'buffer': [], 'dependencies': [(2, 1), (2, 3), (0, 2)]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_dataset(sentences, heads):\n",
        "    \"\"\"Clean and validate the dataset.\"\"\"\n",
        "    cleaned_sentences = []\n",
        "    cleaned_heads = []\n",
        "    skipped_indices = []\n",
        "\n",
        "    for i, (sentence, head) in enumerate(zip(sentences, heads)):\n",
        "        try:\n",
        "            head_as_integers = []\n",
        "            for h in head:\n",
        "                head_as_integers.append(int(h))\n",
        "            head = head_as_integers\n",
        "\n",
        "            head_indices_are_valid = True\n",
        "            for h in head:\n",
        "                if h < 0 or h > len(sentence):\n",
        "                    head_indices_are_valid = False\n",
        "                    break\n",
        "            if not head_indices_are_valid:\n",
        "                skipped_indices.append(i)\n",
        "                continue\n",
        "\n",
        "            root_indices = []\n",
        "            for idx, h in enumerate(head):\n",
        "                if h == 0:\n",
        "                    root_indices.append(idx)\n",
        "\n",
        "            if len(root_indices) != 1:\n",
        "                skipped_indices.append(i)\n",
        "                continue\n",
        "\n",
        "            invalid_head_found = False\n",
        "            for h in head:\n",
        "                if h > len(sentence):\n",
        "                    invalid_head_found = True\n",
        "                    break\n",
        "            if invalid_head_found:\n",
        "                skipped_indices.append(i)\n",
        "                continue\n",
        "\n",
        "            cleaned_sentences.append(sentence)\n",
        "            cleaned_heads.append(head)\n",
        "\n",
        "        except Exception as e:\n",
        "            skipped_indices.append(i)\n",
        "\n",
        "    if len(cleaned_sentences) == 0:\n",
        "        print(\"\\nExample of first few skipped sentences:\")\n",
        "        for idx in skipped_indices[:3]:\n",
        "            print(f\"\\nSentence {idx}:\")\n",
        "            print(f\"Text: {sentences[idx]}\")\n",
        "            print(f\"Heads: {heads[idx]}\")\n",
        "\n",
        "    return cleaned_sentences, cleaned_heads, skipped_indices"
      ],
      "metadata": {
        "id": "LyhC50s7EW45"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_train_data(dataset):\n",
        "    train_data = dataset['train']\n",
        "    sentences = train_data['tokens']\n",
        "    heads = train_data['head']\n",
        "\n",
        "    if not isinstance(sentences[0], list):\n",
        "        sentences = [list(s) for s in sentences]\n",
        "    if not isinstance(heads[0], list):\n",
        "        heads = [list(h) for h in heads]\n",
        "    return sentences, heads\n",
        "\n",
        "def extract_val_data(dataset):\n",
        "    val_data = dataset['validation']\n",
        "    sentences = val_data['tokens']\n",
        "    heads = val_data['head']\n",
        "    if not isinstance(sentences[0], list):\n",
        "        sentences = [list(s) for s in sentences]\n",
        "    if not isinstance(heads[0], list):\n",
        "        heads = [list(h) for h in heads]\n",
        "    return sentences, heads\n",
        "\n",
        "def extract_test_data(dataset):\n",
        "    test_data = dataset['test']\n",
        "    sentences = test_data['tokens']\n",
        "    heads = test_data['head']\n",
        "    if not isinstance(sentences[0], list):\n",
        "        sentences = [list(s) for s in sentences]\n",
        "    if not isinstance(heads[0], list):\n",
        "        heads = [list(h) for h in heads]\n",
        "    return sentences, heads"
      ],
      "metadata": {
        "id": "r9mzDipCEZZB"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process full datasets\n",
        "train_sentences, train_heads = extract_train_data(english_data)\n",
        "clean_train_sent, clean_train_heads, skipped_train = clean_dataset(train_sentences, train_heads)\n",
        "\n",
        "val_sentences, val_heads = extract_val_data(english_data)\n",
        "clean_val_sent, clean_val_heads, skipped_val = clean_dataset(val_sentences, val_heads)\n",
        "\n",
        "test_sentences, test_heads = extract_test_data(english_data)\n",
        "clean_test_sent, clean_test_heads, skipped_test = clean_dataset(test_sentences, test_heads)\n",
        "\n",
        "print(\"\\nFinal Dataset Sizes:\")\n",
        "print(f\"Train: {len(clean_train_sent)} sentences\")\n",
        "print(f\"Validation: {len(clean_val_sent)} sentences\")\n",
        "print(f\"Test: {len(clean_test_sent)} sentences\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OUp41SnEe7g",
        "outputId": "8653f60a-fbad-4e5a-c535-7f2e9d04c397"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Dataset Sizes:\n",
            "Train: 10447 sentences\n",
            "Validation: 1722 sentences\n",
            "Test: 1790 sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with just first few examples\n",
        "sample_sentences = train_sentences[:5]\n",
        "sample_heads = train_heads[:5]\n",
        "\n",
        "print(\"Testing cleaning with first 5 sentences:\")\n",
        "clean_test_sent, clean_test_heads, skipped = clean_dataset(sample_sentences, sample_heads)\n",
        "\n",
        "for i, (sent, heads) in enumerate(zip(clean_test_sent, clean_test_heads)):\n",
        "    print(f\"\\nSentence {i}:\")\n",
        "    print(f\"Text: {sent}\")\n",
        "    print(f\"Heads: {heads}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO0h05waEcOK",
        "outputId": "3f3629c9-45e9-4e10-b16f-4240c6112a75"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing cleaning with first 5 sentences:\n",
            "Processed 5 sentences\n",
            "Kept 5 sentences\n",
            "Skipped 0 sentences\n",
            "\n",
            "Sentence 0:\n",
            "Text: ['Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al', '-', 'Ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'Qaim', ',', 'near', 'the', 'Syrian', 'border', '.']\n",
            "Heads: [0, 1, 1, 1, 6, 7, 1, 7, 8, 8, 8, 8, 8, 15, 8, 18, 18, 7, 21, 21, 18, 23, 21, 21, 28, 28, 28, 21, 1]\n",
            "\n",
            "Sentence 1:\n",
            "Text: ['[', 'This', 'killing', 'of', 'a', 'respected', 'cleric', 'will', 'be', 'causing', 'us', 'trouble', 'for', 'years', 'to', 'come', '.', ']']\n",
            "Heads: [10, 3, 10, 7, 7, 7, 3, 10, 10, 0, 10, 10, 14, 10, 16, 14, 10, 10]\n",
            "\n",
            "Sentence 2:\n",
            "Text: ['DPA', ':', 'Iraqi', 'authorities', 'announced', 'that', 'they', 'had', 'busted', 'up', '3', 'terrorist', 'cells', 'operating', 'in', 'Baghdad', '.']\n",
            "Heads: [0, 1, 4, 5, 1, 9, 9, 9, 5, 9, 13, 13, 9, 13, 16, 14, 1]\n",
            "\n",
            "Sentence 3:\n",
            "Text: ['Two', 'of', 'them', 'were', 'being', 'run', 'by', '2', 'officials', 'of', 'the', 'Ministry', 'of', 'the', 'Interior', '!']\n",
            "Heads: [6, 3, 1, 6, 6, 0, 9, 9, 6, 12, 12, 9, 15, 15, 12, 6]\n",
            "\n",
            "Sentence 4:\n",
            "Text: ['The', 'MoI', 'in', 'Iraq', 'is', 'equivalent', 'to', 'the', 'US', 'FBI', ',', 'so', 'this', 'would', 'be', 'like', 'having', 'J.', 'Edgar', 'Hoover', 'unwittingly', 'employ', 'at', 'a', 'high', 'level', 'members', 'of', 'the', 'Weathermen', 'bombers', 'back', 'in', 'the', '1960s', '.']\n",
            "Heads: [2, 6, 4, 2, 6, 0, 10, 10, 10, 6, 6, 17, 15, 15, 6, 17, 15, 22, 18, 18, 22, 17, 26, 26, 26, 22, 22, 31, 31, 31, 27, 35, 35, 35, 22, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_training_data(sentences, heads):\n",
        "    training_pairs = []\n",
        "    for sent, head in zip(sentences, heads):\n",
        "        try:\n",
        "            actions = dependency_to_actions(sent, head)\n",
        "            training_pair = (sent, actions)\n",
        "            training_pairs.append(training_pair)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            continue\n",
        "\n",
        "    return training_pairs\n"
      ],
      "metadata": {
        "id": "I2kKI1LtGSeD"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OIex9Uv6e6Yh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generating training sequences...\")\n",
        "train_data = generate_training_data(clean_train_sent, clean_train_heads)\n",
        "val_data = generate_training_data(clean_val_sent, clean_val_heads)\n",
        "test_data = generate_training_data(clean_test_sent, clean_test_heads)\n",
        "\n",
        "print(f\"Training sequences: {len(train_data)}\")\n",
        "print(f\"Validation sequences: {len(val_data)}\")\n",
        "print(f\"Test sequences: {len(test_data)}\")\n",
        "\n",
        "print(\"\\nExample training sequence:\")\n",
        "example_sent, example_actions = train_data[0]\n",
        "print(f\"Sentence: {example_sent}\")\n",
        "print(f\"Action sequence: {example_actions}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjmtMh7fFBMW",
        "outputId": "b3ac630d-9cfd-42a5-a720-f826ce1e7daa"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating training sequences...\n",
            "Training sequences: 10447\n",
            "Validation sequences: 1722\n",
            "Test sequences: 1790\n",
            "\n",
            "Example training sequence:\n",
            "Sentence: ['Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al', '-', 'Ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'Qaim', ',', 'near', 'the', 'Syrian', 'border', '.']\n",
            "Action sequence: ['SHIFT', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'SHIFT', 'LEFT-ARC', 'SHIFT', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'RIGHT-ARC', 'RIGHT-ARC', 'SHIFT', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'LEFT-ARC', 'SHIFT', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'LEFT-ARC', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'SHIFT', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'LEFT-ARC', 'LEFT-ARC', 'RIGHT-ARC', 'RIGHT-ARC', 'RIGHT-ARC', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'RIGHT-ARC']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUN0AAksbaTR",
        "outputId": "73a78e9b-c504-4fc9-db19-fc30506b357f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(['Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al', '-', 'Ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'Qaim', ',', 'near', 'the', 'Syrian', 'border', '.'], ['SHIFT', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'SHIFT', 'LEFT-ARC', 'SHIFT', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'RIGHT-ARC', 'RIGHT-ARC', 'SHIFT', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'LEFT-ARC', 'SHIFT', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'LEFT-ARC', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'SHIFT', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'LEFT-ARC', 'LEFT-ARC', 'RIGHT-ARC', 'RIGHT-ARC', 'RIGHT-ARC', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'RIGHT-ARC']), (['[', 'This', 'killing', 'of', 'a', 'respected', 'cleric', 'will', 'be', 'causing', 'us', 'trouble', 'for', 'years', 'to', 'come', '.', ']'], ['SHIFT', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'SHIFT', 'SHIFT', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'LEFT-ARC', 'LEFT-ARC', 'RIGHT-ARC', 'SHIFT', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'LEFT-ARC', 'LEFT-ARC', 'LEFT-ARC', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'RIGHT-ARC', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'RIGHT-ARC']), (['DPA', ':', 'Iraqi', 'authorities', 'announced', 'that', 'they', 'had', 'busted', 'up', '3', 'terrorist', 'cells', 'operating', 'in', 'Baghdad', '.'], ['SHIFT', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'SHIFT', 'LEFT-ARC', 'SHIFT', 'SHIFT', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'LEFT-ARC', 'LEFT-ARC', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'LEFT-ARC', 'SHIFT', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'RIGHT-ARC', 'RIGHT-ARC', 'RIGHT-ARC', 'RIGHT-ARC', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'RIGHT-ARC']), (['Two', 'of', 'them', 'were', 'being', 'run', 'by', '2', 'officials', 'of', 'the', 'Ministry', 'of', 'the', 'Interior', '!'], ['SHIFT', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'RIGHT-ARC', 'SHIFT', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'LEFT-ARC', 'LEFT-ARC', 'SHIFT', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'LEFT-ARC', 'SHIFT', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'LEFT-ARC', 'SHIFT', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'LEFT-ARC', 'RIGHT-ARC', 'RIGHT-ARC', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'RIGHT-ARC']), (['The', 'MoI', 'in', 'Iraq', 'is', 'equivalent', 'to', 'the', 'US', 'FBI', ',', 'so', 'this', 'would', 'be', 'like', 'having', 'J.', 'Edgar', 'Hoover', 'unwittingly', 'employ', 'at', 'a', 'high', 'level', 'members', 'of', 'the', 'Weathermen', 'bombers', 'back', 'in', 'the', '1960s', '.'], ['SHIFT', 'SHIFT', 'LEFT-ARC', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'RIGHT-ARC', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'LEFT-ARC', 'SHIFT', 'SHIFT', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'LEFT-ARC', 'LEFT-ARC', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'SHIFT', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'LEFT-ARC', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'SHIFT', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'LEFT-ARC', 'SHIFT', 'SHIFT', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'LEFT-ARC', 'LEFT-ARC', 'RIGHT-ARC', 'SHIFT', 'SHIFT', 'SHIFT', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'LEFT-ARC', 'LEFT-ARC', 'RIGHT-ARC', 'RIGHT-ARC', 'SHIFT', 'SHIFT', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'LEFT-ARC', 'LEFT-ARC', 'RIGHT-ARC', 'RIGHT-ARC', 'SHIFT'])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_action_sequence(sentence, heads, actions):\n",
        "    \"\"\"\n",
        "    Verify that an action sequence produces the correct dependencies.\n",
        "    \"\"\"\n",
        "    parser = ArcStandardParser()\n",
        "    parser.initialize(sentence)\n",
        "\n",
        "    for action in actions:\n",
        "        parser.parse_step(action)\n",
        "\n",
        "    dependencies = parser.get_state()[\"dependencies\"]\n",
        "\n",
        "    predicted_heads = [0] * len(sentence)\n",
        "    for head_idx, dependent_idx in dependencies:\n",
        "        if head_idx == 0:  # ROOT\n",
        "            predicted_heads[dependent_idx - 1] = 0\n",
        "        else:\n",
        "            predicted_heads[dependent_idx - 1] = head_idx\n",
        "\n",
        "    print(\"Original heads:\", heads)\n",
        "    print(\"Predicted heads:\", predicted_heads)\n",
        "    print(\"Match:\", predicted_heads == heads)\n",
        "    return predicted_heads == heads\n",
        "\n",
        "# Test\n",
        "sentence = ['Al', '-', 'Zaman', ':', 'American', 'forces', 'killed', 'Shaikh', 'Abdullah', 'al', '-', 'Ani', ',', 'the', 'preacher', 'at', 'the', 'mosque', 'in', 'the', 'town', 'of', 'Qaim', ',', 'near', 'the', 'Syrian', 'border', '.']\n",
        "heads = [0, 1, 1, 1, 6, 7, 1, 7, 8, 8, 8, 8, 8, 15, 8, 18, 18, 7, 21, 21, 18, 23, 21, 21, 28, 28, 28, 21, 1]\n",
        "\n",
        "# Test with the example\n",
        "example_sent, example_actions = train_data[0]\n",
        "example_heads = clean_train_heads[0]\n",
        "\n",
        "actions = dependency_to_actions(example_sent, example_heads)\n",
        "verify_action_sequence(example_sent, example_heads, actions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YITU2QIEFMDK",
        "outputId": "023e8e4a-4b8a-43c8-8025-ff0367b14e78"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original heads: [0, 1, 1, 1, 6, 7, 1, 7, 8, 8, 8, 8, 8, 15, 8, 18, 18, 7, 21, 21, 18, 23, 21, 21, 28, 28, 28, 21, 1]\n",
            "Predicted heads: [0, 1, 1, 1, 6, 7, 1, 7, 8, 8, 8, 8, 8, 15, 8, 18, 18, 7, 21, 21, 18, 23, 21, 21, 28, 28, 28, 21, 1]\n",
            "Match: True\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(actions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTfhw4EIWwki",
        "outputId": "9fa9f9ab-599e-4902-bb82-f7fe9c32e411"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['SHIFT', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'SHIFT', 'LEFT-ARC', 'SHIFT', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'RIGHT-ARC', 'RIGHT-ARC', 'SHIFT', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'LEFT-ARC', 'SHIFT', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'LEFT-ARC', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'SHIFT', 'SHIFT', 'SHIFT', 'SHIFT', 'LEFT-ARC', 'LEFT-ARC', 'LEFT-ARC', 'RIGHT-ARC', 'RIGHT-ARC', 'RIGHT-ARC', 'RIGHT-ARC', 'SHIFT', 'RIGHT-ARC', 'RIGHT-ARC']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "metadata": {
        "id": "zUJSttnBS3y4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DependencyParsingDataset(Dataset):\n",
        "    def __init__(self, sentences, heads, tokenizer):\n",
        "        self.sentences = sentences\n",
        "        self.heads = heads\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.actions = []\n",
        "        for s, h in zip(sentences, heads):\n",
        "            action_sequence = dependency_to_actions(s, h)\n",
        "            action_sequence_as_int = self.convert_actions_to_int(action_sequence)\n",
        "            self.actions.append(action_sequence_as_int)\n",
        "\n",
        "    def convert_actions_to_int(self, actions):\n",
        "        action_map = {\n",
        "            \"SHIFT\": 0,\n",
        "            \"LEFT-ARC\": 1,\n",
        "            \"RIGHT-ARC\": 2\n",
        "        }\n",
        "\n",
        "        actions_as_int = []\n",
        "        for action in actions:\n",
        "            actions_as_int.append(action_map[action])\n",
        "\n",
        "        return actions_as_int\n",
        "\n",
        "    def __len__(self):\n",
        "        dataset_length = len(self.sentences)\n",
        "        return dataset_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        actions = self.actions[idx]\n",
        "\n",
        "        sentence_as_string = \" \".join(sentence)\n",
        "        inputs = self.tokenizer(\n",
        "            sentence_as_string,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        token_count = inputs[\"input_ids\"].size(1)\n",
        "\n",
        "        if len(actions) < token_count:\n",
        "            padded_actions = actions + [0] * (token_count - len(actions))\n",
        "            actions = padded_actions\n",
        "        elif len(actions) > token_count:\n",
        "            truncated_actions = actions[:token_count]\n",
        "            actions = truncated_actions\n",
        "\n",
        "        output = {\n",
        "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
        "            \"actions\": torch.tensor(actions, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "1xR4mrHPS4-c"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DependencyParserModel(nn.Module):\n",
        "    def __init__(self, bert_model_name, num_actions):\n",
        "        super(DependencyParserModel, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, num_actions)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        token_outputs = outputs.last_hidden_state\n",
        "        action_logits = self.fc(token_outputs)\n",
        "        return action_logits\n"
      ],
      "metadata": {
        "id": "NZJDkqvlTAKf"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataloader, optimizer, criterion, num_epochs=3):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            actions = batch[\"actions\"].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(input_ids, attention_mask)\n",
        "\n",
        "            batch_size, seq_len, num_actions = logits.size()\n",
        "            logits = logits.view(-1, num_actions)\n",
        "            actions = actions.view(-1)\n",
        "\n",
        "            valid_indices = attention_mask.view(-1).nonzero(as_tuple=True)[0]\n",
        "            logits = logits[valid_indices]\n",
        "            actions = actions[valid_indices]\n",
        "\n",
        "            loss = criterion(logits, actions)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}\")\n"
      ],
      "metadata": {
        "id": "ow4RuvuLTD5N"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            actions = batch[\"actions\"].to(device)\n",
        "\n",
        "            logits = model(input_ids, attention_mask)\n",
        "            predictions = torch.argmax(logits, dim=2)\n",
        "\n",
        "            valid_indices = attention_mask.view(-1).nonzero(as_tuple=True)[0]\n",
        "            predictions = predictions.view(-1)[valid_indices]\n",
        "            actions = actions.view(-1)[valid_indices]\n",
        "\n",
        "            correct += (predictions == actions).sum().item()\n",
        "            total += actions.size(0)\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Evaluation Accuracy: {accuracy:.4f}\")\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "TvPP-GsUTH0Y"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "dataset = DependencyParsingDataset(clean_train_sent, clean_train_heads, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "id": "BjWBzSVJTNGa"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "RvRdbIIQUGGk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_actions = 3\n",
        "model = DependencyParserModel(\"bert-base-uncased\", num_actions).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "lQKrXl2RUBMg"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, batch in enumerate(dataloader):\n",
        "    print(f\"Batch {i + 1}:\")\n",
        "    print(\"Input IDs shape:\", batch[\"input_ids\"].shape)\n",
        "    print(\"Attention Mask shape:\", batch[\"attention_mask\"].shape)\n",
        "    print(\"Actions shape:\", batch[\"actions\"].shape)\n",
        "    print(\"-\" * 40)\n",
        "    if i == 2:  # Limit to first 3 batches\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_M0jYNTZHlM",
        "outputId": "91f15104-74c5-4b00-ecf5-da27e5af96d3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1:\n",
            "Input IDs shape: torch.Size([1, 17])\n",
            "Attention Mask shape: torch.Size([1, 17])\n",
            "Actions shape: torch.Size([1, 17])\n",
            "----------------------------------------\n",
            "Batch 2:\n",
            "Input IDs shape: torch.Size([1, 11])\n",
            "Attention Mask shape: torch.Size([1, 11])\n",
            "Actions shape: torch.Size([1, 11])\n",
            "----------------------------------------\n",
            "Batch 3:\n",
            "Input IDs shape: torch.Size([1, 7])\n",
            "Attention Mask shape: torch.Size([1, 7])\n",
            "Actions shape: torch.Size([1, 7])\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, batch in enumerate(dataloader):\n",
        "    print(f\"Batch {i + 1}:\")\n",
        "    print(\"Input IDs:\", batch[\"input_ids\"])\n",
        "    print(\"Attention Mask:\", batch[\"attention_mask\"])\n",
        "    print(\"Actions:\", batch[\"actions\"])\n",
        "    print(\"-\" * 40)\n",
        "    if i == 2:\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4F0ZXNChBDB",
        "outputId": "82c8d2df-bcda-4b3a-8f48-eaf10f9eefff"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1:\n",
            "Input IDs: tensor([[  101,  2023, 16350, 21500,  8636,  2974,  3084,  2825,  2309,  3529,\n",
            "         23730,  1997, 21500,  3401, 21823,  9289,  2015,  1012,   102]])\n",
            "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "Actions: tensor([[0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 2, 0, 0, 1, 0, 1, 0]])\n",
            "----------------------------------------\n",
            "Batch 2:\n",
            "Input IDs: tensor([[ 101, 9278, 2482, 1004, 3298, 2000, 2034, 3309, 1006, 2625, 2084, 1015,\n",
            "         3178, 2185, 1007, 1025, 7859,  102]])\n",
            "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "Actions: tensor([[0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 2, 0, 0, 1]])\n",
            "----------------------------------------\n",
            "Batch 3:\n",
            "Input IDs: tensor([[  101, 14163,  7377, 11335,  2546,  2253,  2006,  2000,  2377,  4517,\n",
            "         20911, 21530,  2007,  2634,  1999,  2526,  1010, 26875,  2162,  3807,\n",
            "          2008,  2095,  1012,   102]])\n",
            "Attention Mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "Actions: tensor([[0, 0, 1, 0, 2, 0, 0, 1, 0, 0, 1, 2, 0, 0, 1, 2, 0, 0, 1, 2, 0, 2, 0, 0]])\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, dataloader, optimizer, criterion, num_epochs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzJfv5ISUM9q",
        "outputId": "3c93db3c-8c76-4ee5-e832-fb54613075ed"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Loss: 0.25821108132474824\n",
            "Epoch 2/3, Loss: 0.2262984521646665\n",
            "Epoch 3/3, Loss: 0.20295755827890913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(model, dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-pALU4utPxr",
        "outputId": "6dd629ef-dd0f-4229-d4de-61b6b154e789"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Accuracy: 0.8892\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8892249811888638"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4J3QNGRPtSwe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}